{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcc/8WtZ3dtSDkvsorTzDl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathilde-schveitzer/computer_vision/blob/main/RetinaNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1_BSCIulMBXu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kuangliu/pytorch-retinanet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkQJzpWDXHfn",
        "outputId": "cd4351e8-233b-4819-fef3-68c5e3d9379e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-retinanet'...\n",
            "remote: Enumerating objects: 187, done.\u001b[K\n",
            "remote: Total 187 (delta 0), reused 0 (delta 0), pack-reused 187\u001b[K\n",
            "Receiving objects: 100% (187/187), 11.60 MiB | 24.18 MiB/s, done.\n",
            "Resolving deltas: 100% (111/111), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fonctions pour process les données"
      ],
      "metadata": {
        "id": "1IYPbuKlSb4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici ce serait interesting de présenter ce qu'est une anchor plutôt que dans le modèle, et l'histoire des petites boxes"
      ],
      "metadata": {
        "id": "mGSdABmWTCBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from torch import meshgrid\n",
        "from torchvision.ops import box_iou, nms\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import random\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image, ImageDraw"
      ],
      "metadata": {
        "id": "ThA86xP0TUpD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Encode object boxes and labels.'''\n",
        "\n",
        "class DataEncoder:\n",
        "    def __init__(self):\n",
        "        self.anchor_areas = [32*32., 64*64., 128*128., 256*256., 512*512.]  # p3 -> p7\n",
        "        self.aspect_ratios = [1/2., 1/1., 2/1.]\n",
        "        self.scale_ratios = [1., pow(2,1/3.), pow(2,2/3.)]\n",
        "        self.anchor_wh = self._get_anchor_wh()\n",
        "\n",
        "    def _get_anchor_wh(self):\n",
        "        '''Compute anchor width and height for each feature map.\n",
        "        Returns:\n",
        "          anchor_wh: (tensor) anchor wh, sized [#fm, #anchors_per_cell, 2].\n",
        "        '''\n",
        "        anchor_wh = []\n",
        "        for s in self.anchor_areas:\n",
        "            for ar in self.aspect_ratios:  # w/h = ar\n",
        "                h = math.sqrt(s/ar)\n",
        "                w = ar * h\n",
        "                for sr in self.scale_ratios:  # scale\n",
        "                    anchor_h = h*sr\n",
        "                    anchor_w = w*sr\n",
        "                    anchor_wh.append([anchor_w, anchor_h])\n",
        "        num_fms = len(self.anchor_areas)\n",
        "        return torch.Tensor(anchor_wh).view(num_fms, -1, 2)\n",
        "\n",
        "    def _get_anchor_boxes(self, input_size):\n",
        "        '''Compute anchor boxes for each feature map.\n",
        "        Args:\n",
        "          input_size: (tensor) model input size of (w,h).\n",
        "        Returns:\n",
        "          boxes: (list) anchor boxes for each feature map. Each of size [#anchors,4],\n",
        "                        where #anchors = fmw * fmh * #anchors_per_cell\n",
        "        '''\n",
        "        num_fms = len(self.anchor_areas)\n",
        "        fm_sizes = [(input_size/pow(2.,i+3)).ceil() for i in range(num_fms)]  # p3 -> p7 feature map sizes\n",
        "\n",
        "        boxes = []\n",
        "        for i in range(num_fms):\n",
        "            fm_size = fm_sizes[i]\n",
        "            grid_size = input_size / fm_size\n",
        "            fm_w, fm_h = int(fm_size[0]), int(fm_size[1])\n",
        "            xy = meshgrid(fm_w,fm_h) + 0.5  # [fm_h*fm_w, 2]\n",
        "            xy = (xy*grid_size).view(fm_h,fm_w,1,2).expand(fm_h,fm_w,9,2)\n",
        "            wh = self.anchor_wh[i].view(1,1,9,2).expand(fm_h,fm_w,9,2)\n",
        "            box = torch.cat([xy,wh], 3)  # [x,y,w,h]\n",
        "            boxes.append(box.view(-1,4))\n",
        "        return torch.cat(boxes, 0)\n",
        "\n",
        "    def encode(self, boxes, labels, input_size):\n",
        "        '''Encode target bounding boxes and class labels.\n",
        "        We obey the Faster RCNN box coder:\n",
        "          tx = (x - anchor_x) / anchor_w\n",
        "          ty = (y - anchor_y) / anchor_h\n",
        "          tw = log(w / anchor_w)\n",
        "          th = log(h / anchor_h)\n",
        "        Args:\n",
        "          boxes: (tensor) bounding boxes of (xmin,ymin,xmax,ymax), sized [#obj, 4].\n",
        "          labels: (tensor) object class labels, sized [#obj,].\n",
        "          input_size: (int/tuple) model input size of (w,h).\n",
        "        Returns:\n",
        "          loc_targets: (tensor) encoded bounding boxes, sized [#anchors,4].\n",
        "          cls_targets: (tensor) encoded class labels, sized [#anchors,].\n",
        "        '''\n",
        "        input_size = torch.Tensor([input_size,input_size]) if isinstance(input_size, int) \\\n",
        "                     else torch.Tensor(input_size)\n",
        "        anchor_boxes = self._get_anchor_boxes(input_size)\n",
        "        #boxes = change_box_order(boxes, 'xyxy2xywh')\n",
        "\n",
        "        ious = box_iou(anchor_boxes, boxes, order='xywh')\n",
        "        max_ious, max_ids = ious.max(1)\n",
        "        boxes = boxes[max_ids]\n",
        "\n",
        "        loc_xy = (boxes[:,:2]-anchor_boxes[:,:2]) / anchor_boxes[:,2:]\n",
        "        loc_wh = torch.log(boxes[:,2:]/anchor_boxes[:,2:])\n",
        "        loc_targets = torch.cat([loc_xy,loc_wh], 1)\n",
        "        cls_targets = 1 + labels[max_ids]\n",
        "\n",
        "        cls_targets[max_ious<0.5] = 0\n",
        "        ignore = (max_ious>0.4) & (max_ious<0.5)  # ignore ious between [0.4,0.5]\n",
        "        cls_targets[ignore] = -1  # for now just mark ignored to -1\n",
        "        return loc_targets, cls_targets\n",
        "\n",
        "    def decode(self, loc_preds, cls_preds, input_size):\n",
        "        '''Decode outputs back to bouding box locations and class labels.\n",
        "        Args:\n",
        "          loc_preds: (tensor) predicted locations, sized [#anchors, 4].\n",
        "          cls_preds: (tensor) predicted class labels, sized [#anchors, #classes].\n",
        "          input_size: (int/tuple) model input size of (w,h).\n",
        "        Returns:\n",
        "          boxes: (tensor) decode box locations, sized [#obj,4].\n",
        "          labels: (tensor) class labels for each box, sized [#obj,].\n",
        "        '''\n",
        "        CLS_THRESH = 0.5\n",
        "        NMS_THRESH = 0.5\n",
        "\n",
        "        input_size = torch.Tensor([input_size,input_size]) if isinstance(input_size, int) \\\n",
        "                     else torch.Tensor(input_size)\n",
        "        anchor_boxes = self._get_anchor_boxes(input_size)\n",
        "\n",
        "        loc_xy = loc_preds[:,:2]\n",
        "        loc_wh = loc_preds[:,2:]\n",
        "\n",
        "        xy = loc_xy * anchor_boxes[:,2:] + anchor_boxes[:,:2]\n",
        "        wh = loc_wh.exp() * anchor_boxes[:,2:]\n",
        "        boxes = torch.cat([xy-wh/2, xy+wh/2], 1)  # [#anchors,4]\n",
        "\n",
        "        score, labels = cls_preds.sigmoid().max(1)          # [#anchors,]\n",
        "        ids = score > CLS_THRESH\n",
        "        ids = ids.nonzero().squeeze()             # [#obj,]\n",
        "        keep = nms(boxes[ids], score[ids], threshold=NMS_THRESH)\n",
        "        return boxes[ids][keep], labels[ids][keep]"
      ],
      "metadata": {
        "id": "Ovr3ruqNS3tV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Perform transforms on both PIL image and object boxes.'''\n",
        "\n",
        "def resize(img, boxes, size, max_size=1000):\n",
        "    '''Resize the input PIL image to the given size.\n",
        "    Args:\n",
        "      img: (PIL.Image) image to be resized.\n",
        "      boxes: (tensor) object boxes, sized [#ojb,4].\n",
        "      size: (tuple or int)\n",
        "        - if is tuple, resize image to the size.\n",
        "        - if is int, resize the shorter side to the size while maintaining the aspect ratio.\n",
        "      max_size: (int) when size is int, limit the image longer size to max_size.\n",
        "                This is essential to limit the usage of GPU memory.\n",
        "    Returns:\n",
        "      img: (PIL.Image) resized image.\n",
        "      boxes: (tensor) resized boxes.\n",
        "    '''\n",
        "    w, h = img.size\n",
        "    if isinstance(size, int):\n",
        "        size_min = min(w,h)\n",
        "        size_max = max(w,h)\n",
        "        sw = sh = float(size) / size_min\n",
        "        if sw * size_max > max_size:\n",
        "            sw = sh = float(max_size) / size_max\n",
        "        ow = int(w * sw + 0.5)\n",
        "        oh = int(h * sh + 0.5)\n",
        "    else:\n",
        "        ow, oh = size\n",
        "        sw = float(ow) / w\n",
        "        sh = float(oh) / h\n",
        "    return img.resize((ow,oh), Image.BILINEAR), \\\n",
        "           boxes*torch.Tensor([sw,sh,sw,sh])\n",
        "\n",
        "def random_crop(img, boxes):\n",
        "    '''Crop the given PIL image to a random size and aspect ratio.\n",
        "    A crop of random size of (0.08 to 1.0) of the original size and a random\n",
        "    aspect ratio of 3/4 to 4/3 of the original aspect ratio is made.\n",
        "    Args:\n",
        "      img: (PIL.Image) image to be cropped.\n",
        "      boxes: (tensor) object boxes, sized [#ojb,4].\n",
        "    Returns:\n",
        "      img: (PIL.Image) randomly cropped image.\n",
        "      boxes: (tensor) randomly cropped boxes.\n",
        "    '''\n",
        "    success = False\n",
        "    for attempt in range(10):\n",
        "        area = img.size[0] * img.size[1]\n",
        "        target_area = random.uniform(0.56, 1.0) * area\n",
        "        aspect_ratio = random.uniform(3. / 4, 4. / 3)\n",
        "\n",
        "        w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
        "        h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
        "\n",
        "        if random.random() < 0.5:\n",
        "            w, h = h, w\n",
        "\n",
        "        if w <= img.size[0] and h <= img.size[1]:\n",
        "            x = random.randint(0, img.size[0] - w)\n",
        "            y = random.randint(0, img.size[1] - h)\n",
        "            success = True\n",
        "            break\n",
        "\n",
        "    # Fallback\n",
        "    if not success:\n",
        "        w = h = min(img.size[0], img.size[1])\n",
        "        x = (img.size[0] - w) // 2\n",
        "        y = (img.size[1] - h) // 2\n",
        "\n",
        "    img = img.crop((x, y, x+w, y+h))\n",
        "    boxes -= torch.Tensor([x,y,x,y])\n",
        "    boxes[:,0::2].clamp_(min=0, max=w-1)\n",
        "    boxes[:,1::2].clamp_(min=0, max=h-1)\n",
        "    return img, boxes\n",
        "\n",
        "def center_crop(img, boxes, size):\n",
        "    '''Crops the given PIL Image at the center.\n",
        "    Args:\n",
        "      img: (PIL.Image) image to be cropped.\n",
        "      boxes: (tensor) object boxes, sized [#ojb,4].\n",
        "      size (tuple): desired output size of (w,h).\n",
        "    Returns:\n",
        "      img: (PIL.Image) center cropped image.\n",
        "      boxes: (tensor) center cropped boxes.\n",
        "    '''\n",
        "    w, h = img.size\n",
        "    ow, oh = size\n",
        "    i = int(round((h - oh) / 2.))\n",
        "    j = int(round((w - ow) / 2.))\n",
        "    img = img.crop((j, i, j+ow, i+oh))\n",
        "    boxes -= torch.Tensor([j,i,j,i])\n",
        "    boxes[:,0::2].clamp_(min=0, max=ow-1)\n",
        "    boxes[:,1::2].clamp_(min=0, max=oh-1)\n",
        "    return img, boxes\n",
        "\n",
        "def random_flip(img, boxes):\n",
        "    '''Randomly flip the given PIL Image.\n",
        "    Args:\n",
        "        img: (PIL Image) image to be flipped.\n",
        "        boxes: (tensor) object boxes, sized [#ojb,4].\n",
        "    Returns:\n",
        "        img: (PIL.Image) randomly flipped image.\n",
        "        boxes: (tensor) randomly flipped boxes.\n",
        "    '''\n",
        "    if random.random() < 0.5:\n",
        "        img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        w = img.width\n",
        "        xmin = w - boxes[:,2]\n",
        "        xmax = w - boxes[:,0]\n",
        "        boxes[:,0] = xmin\n",
        "        boxes[:,2] = xmax\n",
        "    return img, boxes\n",
        "\n",
        "def draw(img, boxes):\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    for box in boxes:\n",
        "        draw.rectangle(list(box), outline='red')\n",
        "    img.show()\n",
        "\n",
        "\n",
        "def test():\n",
        "    img = Image.open('./image/000001.jpg')\n",
        "    boxes = torch.Tensor([[48, 240, 195, 371], [8, 12, 352, 498]])\n",
        "    img, boxes = random_crop(img, boxes)\n",
        "    print(img.size)\n",
        "    draw(img, boxes)"
      ],
      "metadata": {
        "id": "506yZatOTiHA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Load image/labels/boxes from an annotation file.\n",
        "The list file is like:\n",
        "    img.jpg xmin ymin xmax ymax label xmin ymin xmax ymax label ...\n",
        "'''\n",
        "\n",
        "class ListDataset(data.Dataset):\n",
        "    def __init__(self, root, list_file, train, transform, input_size):\n",
        "        '''\n",
        "        Args:\n",
        "          root: (str) ditectory to images.\n",
        "          list_file: (str) path to index file.\n",
        "          train: (boolean) train or test.\n",
        "          transform: ([transforms]) image transforms.\n",
        "          input_size: (int) model input size.\n",
        "        '''\n",
        "        self.root = root\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "        self.input_size = input_size\n",
        "\n",
        "        self.fnames = []\n",
        "        self.boxes = []\n",
        "        self.labels = []\n",
        "\n",
        "        self.encoder = DataEncoder()\n",
        "\n",
        "        with open(list_file) as f:\n",
        "            lines = f.readlines()\n",
        "            self.num_samples = len(lines)\n",
        "\n",
        "        for line in lines:\n",
        "            splited = line.strip().split()\n",
        "            self.fnames.append(splited[0])\n",
        "            num_boxes = (len(splited) - 1) // 5\n",
        "            box = []\n",
        "            label = []\n",
        "            for i in range(num_boxes):\n",
        "                xmin = splited[1+5*i]\n",
        "                ymin = splited[2+5*i]\n",
        "                xmax = splited[3+5*i]\n",
        "                ymax = splited[4+5*i]\n",
        "                c = splited[5+5*i]\n",
        "                box.append([float(xmin),float(ymin),float(xmax),float(ymax)])\n",
        "                label.append(int(c))\n",
        "            self.boxes.append(torch.Tensor(box))\n",
        "            self.labels.append(torch.LongTensor(label))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''Load image.\n",
        "        Args:\n",
        "          idx: (int) image index.\n",
        "        Returns:\n",
        "          img: (tensor) image tensor.\n",
        "          loc_targets: (tensor) location targets.\n",
        "          cls_targets: (tensor) class label targets.\n",
        "        '''\n",
        "        # Load image and boxes.\n",
        "        fname = self.fnames[idx]\n",
        "        img = Image.open(os.path.join(self.root, fname))\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "\n",
        "        boxes = self.boxes[idx].clone()\n",
        "        labels = self.labels[idx]\n",
        "        size = self.input_size\n",
        "\n",
        "        # Data augmentation.\n",
        "        if self.train:\n",
        "            img, boxes = random_flip(img, boxes)\n",
        "            img, boxes = random_crop(img, boxes)\n",
        "            img, boxes = resize(img, boxes, (size,size))\n",
        "        else:\n",
        "            img, boxes = resize(img, boxes, size)\n",
        "            img, boxes = center_crop(img, boxes, (size,size))\n",
        "\n",
        "        img = self.transform(img)\n",
        "        return img, boxes, labels\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        '''Pad images and encode targets.\n",
        "        As for images are of different sizes, we need to pad them to the same size.\n",
        "        Args:\n",
        "          batch: (list) of images, cls_targets, loc_targets.\n",
        "        Returns:\n",
        "          padded images, stacked cls_targets, stacked loc_targets.\n",
        "        '''\n",
        "        imgs = [x[0] for x in batch]\n",
        "        boxes = [x[1] for x in batch]\n",
        "        labels = [x[2] for x in batch]\n",
        "\n",
        "        h = w = self.input_size\n",
        "        num_imgs = len(imgs)\n",
        "        inputs = torch.zeros(num_imgs, 3, h, w)\n",
        "\n",
        "        loc_targets = []\n",
        "        cls_targets = []\n",
        "        for i in range(num_imgs):\n",
        "            inputs[i] = imgs[i]\n",
        "            loc_target, cls_target = self.encoder.encode(boxes[i], labels[i], input_size=(w,h))\n",
        "            loc_targets.append(loc_target)\n",
        "            cls_targets.append(cls_target)\n",
        "        return inputs, torch.stack(loc_targets), torch.stack(cls_targets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples"
      ],
      "metadata": {
        "id": "qYEcRT9kTQBn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structure du modèle : \n",
        "> Introduire la structure pyramidale \n",
        "\n",
        "> C'est quoi FPN mdr ? En tous cas on en a besoin (en vrai c'est sûrement dans la structure de ce bon vieux Retina)\n",
        "\n",
        "> Bien comprendre ce qu'est une anchor : https://towardsdatascience.com/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9 . Finalement c'est un paramêtre à fine-tune : \n",
        "      # Answer the following question : \n",
        "      # What is the smallest size box I want to be able to detect ?\n",
        "      # What is the largest size box I want to be able to detect ?\n",
        "      # What are the shapes the box can take ?\n",
        "Certains modèles de détection utilise des méthodes comme K-means pour déterminer le \"meilleur\" choix d'anchor.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oJ-e1U5qMikj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.downsample = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.downsample(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FPN(nn.Module):\n",
        "    def __init__(self, block, num_blocks):\n",
        "        super(FPN, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Bottom-up layers\n",
        "        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.conv6 = nn.Conv2d(2048, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv7 = nn.Conv2d( 256, 256, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Lateral layers\n",
        "        self.latlayer1 = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)\n",
        "        self.latlayer2 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n",
        "        self.latlayer3 = nn.Conv2d( 512, 256, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        # Top-down layers\n",
        "        self.toplayer1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.toplayer2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _upsample_add(self, x, y):\n",
        "        '''Upsample and add two feature maps.\n",
        "        Args:\n",
        "          x: (Variable) top feature map to be upsampled.\n",
        "          y: (Variable) lateral feature map.\n",
        "        Returns:\n",
        "          (Variable) added feature map.\n",
        "        Note in PyTorch, when input size is odd, the upsampled feature map\n",
        "        with `F.upsample(..., scale_factor=2, mode='nearest')`\n",
        "        maybe not equal to the lateral feature map size.\n",
        "        e.g.\n",
        "        original input size: [N,_,15,15] ->\n",
        "        conv2d feature map size: [N,_,8,8] ->\n",
        "        upsampled feature map size: [N,_,16,16]\n",
        "        So we choose bilinear upsample which supports arbitrary output sizes.\n",
        "        '''\n",
        "        _,_,H,W = y.size()\n",
        "        return F.upsample(x, size=(H,W), mode='bilinear') + y\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Bottom-up\n",
        "        c1 = F.relu(self.bn1(self.conv1(x)))\n",
        "        c1 = F.max_pool2d(c1, kernel_size=3, stride=2, padding=1)\n",
        "        c2 = self.layer1(c1)\n",
        "        c3 = self.layer2(c2)\n",
        "        c4 = self.layer3(c3)\n",
        "        c5 = self.layer4(c4)\n",
        "        p6 = self.conv6(c5)\n",
        "        p7 = self.conv7(F.relu(p6))\n",
        "        # Top-down\n",
        "        p5 = self.latlayer1(c5)\n",
        "        p4 = self._upsample_add(p5, self.latlayer2(c4))\n",
        "        p4 = self.toplayer1(p4)\n",
        "        p3 = self._upsample_add(p4, self.latlayer3(c3))\n",
        "        p3 = self.toplayer2(p3)\n",
        "        return p3, p4, p5, p6, p7\n",
        "\n",
        "\n",
        "def FPN50():\n",
        "    return FPN(Bottleneck, [3,4,6,3])\n",
        "\n",
        "def FPN101():\n",
        "    return FPN(Bottleneck, [2,4,23,3])"
      ],
      "metadata": {
        "id": "ZwGMohkuQTrQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RetinaNet(nn.Module):\n",
        "    num_anchors = 9 # a parameter to fine-tune\n",
        "    \n",
        "    \n",
        "    def __init__(self, num_classes=20):\n",
        "        super(RetinaNet, self).__init__()\n",
        "        self.fpn = FPN50()\n",
        "        self.num_classes = num_classes\n",
        "        self.loc_head = self._make_head(self.num_anchors*4)\n",
        "        self.cls_head = self._make_head(self.num_anchors*self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        fms = self.fpn(x)\n",
        "        loc_preds = []\n",
        "        cls_preds = []\n",
        "        for fm in fms:\n",
        "            loc_pred = self.loc_head(fm)\n",
        "            cls_pred = self.cls_head(fm)\n",
        "            loc_pred = loc_pred.permute(0,2,3,1).contiguous().view(x.size(0),-1,4)                 # [N, 9*4,H,W] -> [N,H,W, 9*4] -> [N,H*W*9, 4]\n",
        "            cls_pred = cls_pred.permute(0,2,3,1).contiguous().view(x.size(0),-1,self.num_classes)  # [N,9*20,H,W] -> [N,H,W,9*20] -> [N,H*W*9,20]\n",
        "            loc_preds.append(loc_pred)\n",
        "            cls_preds.append(cls_pred)\n",
        "        return torch.cat(loc_preds,1), torch.cat(cls_preds,1)\n",
        "\n",
        "    def _make_head(self, out_planes):\n",
        "        layers = []\n",
        "        for _ in range(4):\n",
        "            layers.append(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1))\n",
        "            layers.append(nn.ReLU(True))\n",
        "        layers.append(nn.Conv2d(256, out_planes, kernel_size=3, stride=1, padding=1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def freeze_bn(self):\n",
        "        '''Freeze BatchNorm layers.'''\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.BatchNorm2d):\n",
        "                layer.eval()"
      ],
      "metadata": {
        "id": "Vxh2XSusMMRg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss \n",
        "\n",
        "> Introduire un booléen qui permettrait de comparer ce qu'on obtient avec une loss L1 et une loss Focal loss"
      ],
      "metadata": {
        "id": "jYQGGBxCO3cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "d_hJKnWwO51X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, num_classes=20):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def focal_loss(self, x, y):\n",
        "        '''Focal loss.\n",
        "        Args:\n",
        "          x: (tensor) sized [N,D].\n",
        "          y: (tensor) sized [N,].\n",
        "        Return:\n",
        "          (tensor) focal loss.\n",
        "        '''\n",
        "        alpha = 0.25\n",
        "        gamma = 2\n",
        "\n",
        "        t = F.one_hot(y.data, 1+self.num_classes)  # [N,21]\n",
        "        t = t[:,1:]  # exclude background\n",
        "        t = Variable(t).cuda()  # [N,20]\n",
        "\n",
        "        p = x.sigmoid()\n",
        "        pt = p*t + (1-p)*(1-t)         # pt = p if t > 0 else 1-p\n",
        "        w = alpha*t + (1-alpha)*(1-t)  # w = alpha if t > 0 else 1-alpha\n",
        "        w = w * (1-pt).pow(gamma)\n",
        "        return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)\n",
        "\n",
        "    def focal_loss_alt(self, x, y):\n",
        "        '''Focal loss alternative.\n",
        "        Args:\n",
        "          x: (tensor) sized [N,D].\n",
        "          y: (tensor) sized [N,].\n",
        "        Return:\n",
        "          (tensor) focal loss.\n",
        "        '''\n",
        "        alpha = 0.25\n",
        "\n",
        "        t = F.one_hot(y.data, 1+self.num_classes)\n",
        "        t = t[:,1:]\n",
        "        t = Variable(t).cuda()\n",
        "\n",
        "        xt = x*(2*t-1)  # xt = x if t > 0 else -x\n",
        "        pt = (2*xt+1).sigmoid()\n",
        "\n",
        "        w = alpha*t + (1-alpha)*(1-t)\n",
        "        loss = -w*pt.log() / 2\n",
        "        return loss.sum()\n",
        "\n",
        "    def forward(self, loc_preds, loc_targets, cls_preds, cls_targets):\n",
        "        '''Compute loss between (loc_preds, loc_targets) and (cls_preds, cls_targets).\n",
        "        Args:\n",
        "          loc_preds: (tensor) predicted locations, sized [batch_size, #anchors, 4].\n",
        "          loc_targets: (tensor) encoded target locations, sized [batch_size, #anchors, 4].\n",
        "          cls_preds: (tensor) predicted class confidences, sized [batch_size, #anchors, #classes].\n",
        "          cls_targets: (tensor) encoded target labels, sized [batch_size, #anchors].\n",
        "        loss:\n",
        "          (tensor) loss = SmoothL1Loss(loc_preds, loc_targets) + FocalLoss(cls_preds, cls_targets).\n",
        "        '''\n",
        "        batch_size, num_boxes = cls_targets.size()\n",
        "        pos = cls_targets > 0  # [N,#anchors]\n",
        "        num_pos = pos.data.long().sum()\n",
        "\n",
        "        ################################################################\n",
        "        # loc_loss = SmoothL1Loss(pos_loc_preds, pos_loc_targets)\n",
        "        ################################################################\n",
        "        mask = pos.unsqueeze(2).expand_as(loc_preds)       # [N,#anchors,4]\n",
        "        masked_loc_preds = loc_preds[mask].view(-1,4)      # [#pos,4]\n",
        "        masked_loc_targets = loc_targets[mask].view(-1,4)  # [#pos,4]\n",
        "        loc_loss = F.smooth_l1_loss(masked_loc_preds, masked_loc_targets, size_average=False)\n",
        "\n",
        "        ################################################################\n",
        "        # cls_loss = FocalLoss(loc_preds, loc_targets)\n",
        "        ################################################################\n",
        "        pos_neg = cls_targets > -1  # exclude ignored anchors\n",
        "        mask = pos_neg.unsqueeze(2).expand_as(cls_preds)\n",
        "        masked_cls_preds = cls_preds[mask].view(-1,self.num_classes)\n",
        "        cls_loss = self.focal_loss_alt(masked_cls_preds, cls_targets[pos_neg])\n",
        "\n",
        "        print('loc_loss: %.3f | cls_loss: %.3f' % (loc_loss.data[0]/num_pos, cls_loss.data[0]/num_pos), end=' | ')\n",
        "        loss = (loc_loss+cls_loss)/num_pos\n",
        "        return loss"
      ],
      "metadata": {
        "id": "gidFSg3bO-YA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boucle d'entraînement\n"
      ],
      "metadata": {
        "id": "Vc9KH6bgPhkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "#from datagen import ListDataset\n",
        "\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "He62gWEXPlke"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert torch.cuda.is_available(), 'Error: CUDA not found!'\n",
        "best_loss = float('inf')  # best test loss\n",
        "start_epoch = 0  # start from epoch 0 or last epoch"
      ],
      "metadata": {
        "id": "2UbDNhPJPyny"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485,0.456,0.406), (0.229,0.224,0.225))\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEdQdfNEXguk",
        "outputId": "3639fa39-8cae-4585-ab3f-963f6f8cdef7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = ListDataset(root='/pytorch-retinanet/data/voc_all_images', #pb vient du '/'\n",
        "                       list_file='./data/voc12_train.txt', train=True, transform=transform, input_size=600)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True, num_workers=8, collate_fn=trainset.collate_fn)\n",
        "\n",
        "testset = ListDataset(root='/search/odin/liukuang/data/voc_all_images',\n",
        "                      list_file='./data/voc12_val.txt', train=False, transform=transform, input_size=600)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=False, num_workers=8, collate_fn=testset.collate_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "aRkYdWS4R7Z9",
        "outputId": "f84e6067-5f06-453f-e2c3-13e9acbed643"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-17ebecebd2f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainset = ListDataset(root='/search/odin/liukuang/data/voc_all_images',\n\u001b[0m\u001b[1;32m      2\u001b[0m                        list_file='./data/voc12_train.txt', train=True, transform=transform, input_size=600)\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m testset = ListDataset(root='/search/odin/liukuang/data/voc_all_images',\n",
            "\u001b[0;32m<ipython-input-20-316a39b6c1b3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, list_file, train, transform, input_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/voc12_train.txt'"
          ]
        }
      ]
    }
  ]
}